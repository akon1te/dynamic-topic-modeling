{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaModel\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_file = '/mnt/d/Datasets/superseg/segmentation_file_train.json'\n",
    "val_json_file = '/mnt/d/Datasets/superseg/segmentation_file_validation.json'\n",
    "with open(train_json_file, 'r') as train_f:\n",
    "    train_ds = json.load(train_f)['dial_data']['superseg-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringPipeline:\n",
    "    def __init__(self, tokenizer, feature_extractor, reducer, clustering):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.reducer = reducer\n",
    "        self.clustering = clustering  \n",
    "\n",
    "    def fit_predict(self, dialog, topics):\n",
    "        self.topics = topics\n",
    "        input_ids = [self.tokenizer.encode(i, return_tensors='pt') for i in dialog]\n",
    "        \n",
    "        sentence_vectors = []\n",
    "        with torch.no_grad():\n",
    "            for item in input_ids:\n",
    "                output_states = self.feature_extractor(item).last_hidden_state[0][0]\n",
    "                sentence_vectors.append(output_states.detach().numpy())\n",
    "                \n",
    "        reduced_vector = self.reducer.fit_transform(np.array(sentence_vectors))\n",
    "        \n",
    "        self.clustering.fit(reduced_vector)\n",
    "        self.predicted_labels_ = self.clustering.labels_\n",
    "        self.compute_metrics(senteces=self.predicted_labels_, topics_id=topics)\n",
    "        \n",
    "        return self.predicted_labels_\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_dialseg(data):\n",
    "        dialogs = []\n",
    "        topics = []\n",
    "        for item in tqdm(data):\n",
    "            dialogs.append([turn['utterance'] for turn in item['turns']])\n",
    "            topics.append([turn['topic_id'] for turn in item['turns']])\n",
    "        return dialogs, topics\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_metrics(senteces, topics_id):\n",
    "        print(f'Adjusted rand score: {adjusted_rand_score(labels_true=topics_id, labels_pred=senteces)}')\n",
    "        print(f'Adjusted mutual indo score: {adjusted_mutual_info_score(labels_true=topics_id, labels_pred=senteces)}')\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()\n",
    "\n",
    "umap_reducer = UMAP(n_components=15, n_neighbors=3)\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=2)\n",
    "\n",
    "pipeline = ClusteringPipeline(tokenizer=tokenizer, feature_extractor=model, reducer=umap_reducer, clustering=hdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6948/6948 [00:00<00:00, 241690.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dialogs, topics = pipeline.preprocess_dialseg(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted rand score: 0.010494752623688156\n",
      "Adjusted mutual indo score: 0.07993635285827581\n",
      "True lables:  [0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2]\n",
      "Predicted labels:  [1 1 0 0 0 2 0 2 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "pred_labels = pipeline.fit_predict(dialog=dialogs[index], topics=topics[index])\n",
    "print(\"True lables: \", topics[index])\n",
    "print(\"Predicted labels: \", pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
