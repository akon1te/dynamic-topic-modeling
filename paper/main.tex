\documentclass[PMI,VKR]{HSEUniversity}
% Возможные опции: KR или VKR; PI или BI

\title{Dynamic Topic Modeling for Voice Dialogues}
\author{Miakov Timofey Ilich}
\supervisor{Professor NRU HSE - Nizhniy Novgorod}{L.V. Savchenko}
\Year{2024}
\Abstract{}

\bibliography{library.bib}

\begin{document}
\maketitle

\chapter{Introduction}



\section{Relevance of the topic}



\section{Research Objectives}

The main objectives of the research work:
\begin{enumerate}
    \item Investigate existing solutions for dynamic text and dialogue topic modelling
    \item The definition of the main stages for DTM for audio dialogues
    \item The identification of suitable training data sets and their collection
    \item An exploration of the possibilities of using large language models for some stages and the selection of models suitable for training in limited resources
    \item The development of the architecture of all stages and their interconnections
    \item Selection of appropriate metrics and measurement of the quality of the approaches used at each stage
    \item Visual analysis of the pipeline operation on pre-selected audio data
\end{enumerate}


\chapter{Literature review}

\section{Transformer architecture}

The main architecture for working with text was Recurrent Neural Networks. However, this approach has several well-known disadvantages:

First of all, RN processes tokens and updates the hidden state sequentially, with this approach, the model begins to forget what happened before or it has to have a huge hidden state.
Secondly, it is very difficult to start distributed training of recurrent networks, because the hidden state at each step must be considered separately.

In order to avoid the issues previously outlined, the article "Attention is all you need" \cite{attention:2017} presented the Transformer architecture and its principal component, 
the Attention Mechanism. The objective of this methodology is to permit the elements of the sequence to be accessed at any time, thereby reducing the loss of information.

\subsection{Self-attention mechanism}

Self-attention represents a key component of the model in which tokens' information interact with each other.
At each stage of the encoder, tokens exchange information with one another, gather contextual data, and refine their previous representations (Figure 2.3). 

For each input token trainable projections $W_Q$, $W_K$, $W_V$ obtained three representations interpreted by humans as a query ($q_i$), a key ($k_i$), and a value ($v_i$).
\begin{itemize}
    \item $q_i$ - query for other tokens about the information;
    \item $k_i$ - keys that will be used to extract information about the token;
    \item $v_i$ - values of information;
\end{itemize}

Next are the attention weights that will be applied to the values from which the important information will be extracted:
\begin{center}
    $Attn_Weights_{i} = softmax(\frac{q_i \cdot k_{1}^T}{C}, \frac{q_i \cdot k_{2}^T}{C}, \dots)$,
\end{center}

where $C$ is some normalization constant.

The Self-Attention result is a weighted sum of Values with coefficients in the form of attention weights. In vectorized form, you can write:

\begin{center}
    $Attention(q, k, v) = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V $
\end{center}
where $\sqrt[]{d}$ constant of the dimension of $K$ and $V$. 

The figure below shows the complete algorithm for the interaction of Q, K, V.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/query-key-value.png}
    \caption{Query, Key, Value in Self-Attention mechanism}
\end{figure}

In the decodert part the structure of the attention mechanism is different. 
They use Masked self-attention. In the form of attention described above, each token will "look" at the entire sequence, which is undesirable for the decoder. 

In the decoder, the generation of tokens occurs sequentially. If the attention mechanism has access to all tokens, not just those already used, this will result in information leakage in the decoder at the training stage and a reduction in the quality of the model.
To avoid this problem, Masked Self-Attention is used, which masks tokens that follow the current one and reduces their probability in the softmax function to 0.

\subsection{Multi-head attention}

To cover more information and dependencies, the authors propose the use of multiple levels of attention in parallel and the summarisation of the values obtained. 
This is referred to as Multi-head attention. The figure below illustrates what a multi-head approach looks like:

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{img/multi-head.png}
    \caption{Query, Key, Value in Self-Attention mechanism}
\end{figure}

\subsection{Feed Forward Network}

Transformer block also contains feed-forward network consisting of a linear layer and a activation function.
The FFN layer can be expressed as follows:
\begin{center}
    $FFN(x) = act(x \dot W_{1} + b_{1}) \dot W_{2} + b_{2}$, \\    
\end{center}
where $act$ is the activation function such as ReLU = $\max(0, x)$ or more contemporary GELU = $x\Phi(x)$.

\subsection{LayerNorm}

In terms of LayerNorm (Figure 2.3), it is normalizes the vector representation of each sequence in the batch. 
In addition, LayerNorm has trainable parameters, $scale$ and $bias$, which are used after normalization to scale the output of the layer.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/layer_norm.png}
    \caption{LayerNorm слой}
\end{figure}

The application of layer normalisation ensures that all parameters within a given layer exhibit a uniform distribution across all features for a specific input.
The LayerNorm can be expressed as:
\begin{center}
    $y = \frac{x - \mu}{\sqrt{\sigma^{2} + \epsilon}} \odot \gamma + \beta$,
\end{center}
where $\mu$ - mean of last D dimension, $\sigma^2$ - variance of last D dimension, $\gamma$ and $\beta$ - learnable params;
      

\subsection{Positional encodings and Rotary embedding}

A sequence of tokens is fed into the model, but the model itself does not understand in which order the tokens are located. 
Therefore, positional embeddings are added to the tokens, which are designed to show the model the input order of the elements.
Positional embeddings are given by formulas:
\begin{center}
    $PE_{position, 2i} = \sin(\frac{position}{10000^{2i/d_{model}}})$, \\
    $PE_{position, 2i+1} = \cos(\frac{position}{10000^{2i/d_{model}}})$, \\
\end{center}

Each measure of position encoding corresponds to a sin wave, and the wavelengths form a geometric progression from $2\pi$ to 10,000$\cdot 2\pi$.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/pos_encoding.png}
    \caption{Positional encoding}
\end{figure}

A newer version of positional encoding is Rotary Positional Embeddings.
Rotary position embedding represents a distinct approach to incorporating relative position information into the attention matrix. 
This approach differs from other approaches in that it first multiplies queries and keys with a rotation matrix. This rotation matrix is a function of absolute position. 
Calculating the inner products of rotated queries $W_Q$ and keys $W_K$ results in an attention matrix that is a function of relative position information only.

In order to illustrate the concept, we will consider an examples with two features at position $m$.
\begin{center}
    $RoPE(x_1, x_2) =   \begin{pmatrix}
                            \cos m\theta & -\sin m\theta\\
                            \sin\theta & \cos m\theta
                        \end{pmatrix} 
                        \begin{pmatrix}
                            x_1 \\
                            x_2
                        \end{pmatrix} 
                        \begin{pmatrix}
                            x_1 \cos m\theta - x_2\sin m\theta\\
                            x_2 \cos m\theta + x_1\sin m\theta
                        \end{pmatrix},
    $
\end{center}
where $\theta$ is a constant angle.

\section{BERT}
\subsection{Model overview}

The BERT is a language model based on the transformer architecture, that was first introduced in the article BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\cite{bert:2018}.  
From an architectural point of view, BERT consists of 12 encoder blocks, including a Self-Attention mechanism, Normalization and Feed-forward layers.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{img/bert.png}
    \caption{BERT architecture}
\end{figure}


\subsection{Training tasks}
BERT has two pretrain objective described in article: \\
The first objective of the training process is the masked language modelling (MLM). 
During the training phase of MLM, the following occurs:

\begin{itemize}
    \item A number of tokens are selected with a probability of 15\% for each token.
    \item The selected tokens are replaced by [MASK] with a probability of 80\%, a random token with a probability of 15\% and remain unchanged with a probability of 10\%.
    \item The model must predict the original token.
\end{itemize}

Secondly, Next Sentence Prediction (NSP) pretraining task is a binary classification task. 
In order to understand relationship between two sentences, BERT training process also uses next sentence prediction.
During training the model gets as input pairs of sentences and it learns to predict if the second sentence is the next sentence in the original text as well. 
The training dataset presented in the original paper states that when trained, 50\% of the examples contain related sentences extracted from the training texts, while the remaining 50\% contain a random pair of sentences.


\section{LLM}

LLM is a type of artificial intelligence(AI) algorithm that use deep learning techniques and massively large data sets to understand, summarize, generate, and predict new content. 
LLMs are trained with immense amounts of data and use self-supervised learning to predict the next token in a sentence, given the surrounding context. 
Once an LLM has been trained, it can be fine-tuned for a wide range of NLP tasks, including generating and classifying text, answering questions.
The following section will examine the main LLMs used in the article.

\subsection{LLaMA}

In July 2023, an open-source collection of models, called LLaMA 2, was released by Mata AI Team. The collection consists of models with 7B and 70B parameters.

In order to create a collection of LLaMA 2 models, the authors employed the architecture of an optimised auto-regressive transformer (Touvron et al.(2023)) with RMSNorm normalisation, Rotary Embeddings and SwiGLU activation function in FFN, but implemented a number of alterations with the intention of enhancing the quality of the model and its performance, such as:
\begin{itemize}
    \item Meticulous data preparation
    \item Larger dataset
    \item Increased the length of the context in compare with earlier approaches
    \item Incorporation of grouped-query attention (GQA)
\end{itemize}   

\textbf{Grouped-query attention} can be thought of as a way to optimize the attention mechanism in transformer-based models. 
Instead of computing attention for each query independently, GQA groups queries together and computes their attention jointly. This reduces the number of attention computations, leading to faster inference times.

The GQA method has been applied to speed up inference on large language models without significantly sacrificing quality. 
It's a promising technique for improving the efficiency of transformer models, particularly in the context of generative AI.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/gqa.png}
    \caption{Self-attention and Grouped-query attention}
\end{figure}


\subsection{Mistral}

In September 2023, the MistralAI team introduced the Mistral 7B model, which has been designed for high performance and efficiency. 
In comparison to the 13B Llama 2, the Mistral 7B model has demonstrated superior performance in all benchmarks related to reasoning, mathematics, and code generation. 
The Mistral 7B model employs grouped-query attention (GQA) for rapid inference and sliding window attention (SWA) for handling long sequences at a reduced cost.

As other models, Mistral is a transformer based model. However, it differs from the Llama model in several key respects.
\begin{itemize}
    \item Sliding Window Attention (SWA): This method allows the model to attend to information beyond a set window size by leveraging the stacked layers of a transformer. 
    With a window size of 4096, it can attend to around 131K tokens. 
    \item Rolling Buffer Cache: It is limits the size of the cache using a rolling buffer. The cache has a fixed size, and as new data is added, older data is overwritten once the cache exceeds its size. 
    This method reduces the memory usage of the cache by 8x for sequences of 32k tokens without compromising the quality of the model.
    \item In the context of sequence generation, the model employs a process of pre-filling and chunking. 
    This involves the prediction of tokens in a sequential manner, given that each token is contingent upon the previous one. However, given that the prompts are known in advance, the cache can be pre-filled with them. 
    In the event that a prompt is of a considerable length, it can be divided into smaller units, with the cache being filled in this manner. 
    The attention mask operates over both the cache and the chunk
\end{itemize}


The pipeline of dynamic thematic modelling of audio dialogues, which will be considered in this paper, is divided into several main stages. 
These include audio-to-text, dialogue segmentation, topic extraction, and topic evolution. 
The approaches and algorithms used in these stages will be described in detail from a theoretical point of view.

\section{Automatic Speech Recognition Stage}


\section{Dialogue Segmentation Stage}

\subsection{Utterance-Pair Coherence Scoring approach}
The dialogue segmentation stage represents the initial stage for textual type of data. 
The result is a set of utterance indices that indicate the beginning of a thematically related group. 

In accordance with the majority of previous studies, we have utilise the best open-source solution based on the presented in papers metrics. 
It is algorithm from Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring by Linzi Xing and Giuseppe Carenini (2020)
Let us now examine the algorithm in greater detail. 

In terms of our approach to topic segmentation, authors utilise Next Sentence Prediction (NSP) BERT as the encoder, due to the similar type of task. 
Input of both tasks an utterances, with the objective of predicting the appropriate next sentence, which should be topically related. 

In more detail, the positive instances $(s_{i}, s_{t+i})$ and negative instances $(s_{i}, s_{t-i})$ use in the model's fine-tuning stage in the form described below: 
\begin{center}
    $[CLS] s_{i} [SEP] s_{t+/-i} [SEP]$,
\end{center}
where the symbols $[CLS]$ and $[SEP]$ represent special tokens in BERT. 

In order to indicate the thematic similarity of two statements in a dialogue, the $[CLS]$ token is employed, which performs a similar role in the usual BERT. 
The resulting value is subjected to a linear dictionary and a final similarity value is obtained, which is then utilised to calculate Coherence scores.

\subsection{Depth Scores}

With the help of a fine-tuned BERT (the training algorithm is described above) coherence scores $c_i$, where $i \in [0, n - 1]$ are calculate. According to texttiling algorithm description $c_i$ indicates the topical relatedness of the two adjacent utterances.
The model uses the obtained values as base values, but processes them for final predictions and gets Depth scores $ds_i$, where $i \in [0, n - 1]$  - measuring the sharpness of a valley.

The depth score is calculated as shows below: 
\begin{center}
    $D_{pi} = \frac{h_{l}(i) + h_{r}(i) - 2 \cdot c_{i}}{2}$,
\end{center}
where $h_{l}, h_{r}$ - the highest cs for $i$ pair in choosen range.

The greater the depth score between two statements, the less likely it is that they share a thematic relationship. 
To identify the pairs along which the boundaries of the thematic segments lie, the threshold value is calculated as follows: $threshold = \mu - \frac{\sigma}{2}$, where $\mu$ is the average value and $\sigma$ is the standard deviation. Pairs with a depth score above the $threshold$ value will be choosen as boundaries.

\subsection{Stage Evaluation}

After defining the basic segmentation algorithm, it is worth talking about evaluation metrics for this stage:
\begin{itemize}
    \item \textbf{F1}\\
    Given that this is a binary classification problem, it is tempting to use Precision and Recall, and their combination $F_{\beta}$ score as an evaluation metric. \\ 
     - Metric is defined as the boundaries ratio identified as true boundaries; \\
       $Precision = \frac{true \: positive}{true \: positive + false \: positive}$ \\
     - Recall is defined as the true boundaries ratio identified by the model. \\
       $Recall = \frac{true \: positive}{true \: positive + false \: negative}$ \\
    $F_{\beta}$ score is defined as the weighted harmonic mean of precision and recall: \\
    $F_{\beta} = (1 + \beta^{2}) \cdot \frac{precision \cdot recall}{(\beta^{2} \cdot precision) + recall}$
    \item \textbf{Pk score} is uses the sliding window method and counts the number of discrepancies between the ends of the predicted segments and the true segments. The final score is the counter divided by the number of measurements. The lower the value, the better.
    \item In \textbf{WindowDiff} this algorithm, for each position of a sliding window of size k, comparing the number of boundaries in the ground truth with the number of boundaries predicted by the Topic Segmentation model.\\
    $WD(pred, true) = \frac{1}{N - k}\sum_{i = 1}^{N - k}(\left\lvert bounds(true_{i}, true_{i + k}) - bounds(pred_{i}, pred_{i + k}) \right\rvert > 0)$, \\ 
    where $bounds(s, s + i)$ represents the number of boundaries between two positions $i$ and $j$ in the text, $N$ represents the number of sentences in the text.
\end{itemize} 

In practice, both $Pk$ and $WindowDiff$ scores are used to evaluate a model. 
A lower score means predictions are closer to the actual boundaries.

\section{Topic Extraction Stage}


\subsection{PromptTopic}


\subsection{LLM for topic extraction}

\subsection{Stage Evaluation}
As for the quality evaluation of the extracted topics, we will use two metrics: Topic coherence and Topic diversity
\begin{itemize}
    \item \textbf{Topic diversity} is a metric that assesses the ratio of unique keywords across all topic representations. This metric is used to measure the capabilities of a topic model covering a wide range of topics. The diversity score is a value between 0 and 1, with a score of 0 indicating repetitive topics and a score of 1 indicating diverse topics.
    \item \textbf{Topic coherence} is a measure of the degree of interconnection of words within a topic. In our experiments, we employed normalized point-to-point mutual information (PMI) as a metric for thematic consistency. A higher score for a given name indicates greater consistency, with an absolute correlation estimated at 1 point.
    \end{itemize}


\section{Topic Evolution over time}






\newpage
\chapter{Experiments}

The experiments will be divided into stages, with each stage accompanied by a description of the stage and the algorithms used. 
This description is included in the theoretical part of the work.


\section{Experiments settings}



\section{ASR experiments}



\section{Text clustering}

The input data at this stage are dialogues divided into separate utterances of the participants ${ut_1, ut_2, \dots, ut_n}$.
At this stage, experiments will be conducted using the encoder model for the Utterance-Coherence Scoring algorithm. 
In addition, algorithms for the time of model inference will be performed throughout the pipeline to select the fastest solution.


\begin{center}
    \begin{tabular}{ | c | c | c | c | c | c | c | c |} 
        \hline
        \multicolumn{2}{| c |}{Country List} & \multicolumn{3}{| c |}{Country List} &  \multicolumn{3}{| c |}{Country List}  \\
        \hline
        model & dataset & Pk & wd & f1 & Pk & wd & f1 \\ 
        \hline
        bert-base NSP & committee & Pk & wd & f1 & Pk & wd & f1 \\ 
        bert-base CLS & committee & Pk & wd & f1 & Pk & wd & f1 \\ 
        bert-large NSP & isci & Pk & wd & f1 & Pk & wd & f1 \\ 
        bert-base CLS & isci & Pk & wd & f1 & Pk & wd & f1 \\ 
        bert-base NSP & ami & Pk & wd & f1 & Pk & wd & f1 \\ 
        bert-base CLS & ami & Pk & wd & f1 & Pk & wd & f1 \\ 
        \hline
    \end{tabular}
\end{center}

\section{Topic extraction}

\begin{center}
    \begin{tabular}{ |c|c|c| } 
     \hline
     model & Pk & wd \\ 
     \hline
     bert-base NSP & cell5 & cell6 \\ 
     bert-base CLS & cell8 & cell9 \\ 
     \hline
    \end{tabular}
\end{center}

\section{Topic Evolution}

\section{Limitations}

The LLM and other models require a significant amount of computing resources, including GPUs. Due to the limited availability of resources, the models were not larger than 8 billion parameters and did not exceed 16 GB during training and inference at all stages. 
Additionally, due to the lack of financial opportunities, open-source models were employed in the work. Should it be possible to reproduce all stages on more advanced models (such as GPT4 or LlaMA2 80b), the experimental results would be considerably more favourable.

\chapter{Conclusion}


\putbibliography
\end{document}