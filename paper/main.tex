\documentclass[PMI,VKR]{HSEUniversity}
% Возможные опции: KR или VKR; PI или BI

\title{Dynamic Topic Modeling for Voice Dialogues}
\author{Miakov Timofey Ilich}
\supervisor{Professor NRU HSE - Nizhniy Novgorod}{L.V. Savchenko}
\Year{2024}
\Abstract{}

\bibliography{library.bib}

\begin{document}
\maketitle

\chapter{Introduction}



\section{Relevance of the topic}



\section{Research Objectives}

The objectives of the thesis work are:
\begin{enumerate}
    \item Research of existing solutions for dynamic text and dialogue topic modelling
    \item The definition of the main stages for DTM for audio dialogues
    \item The identification of suitable training data sets and their collection
    \item An exploration of the possibilities of using large language models for some stages and the selection of models suitable for training in limited resources
    \item The development of the architecture of all stages and their interconnections
    \item Selection of appropriate metrics and measurement of the quality of the approaches used at each stage
    \item Visual analysis of the pipeline operation on pre-selected audio data
\end{enumerate}


\chapter{Literature review}

\section{Transformer architecture}

The main architecture for working with text was Recurrent Neural Networks. However, this approach has several well-known disadvantages:

First of all, RN processes tokens and updates the hidden state sequentially, with this approach, the model begins to forget what happened before or it has to have a huge hidden state.
Secondly, it is very difficult to start distributed training of recurrent networks, because the hidden state at each step must be considered separately.

In order to avoid the issues previously outlined, the article "Attention is all you need" \cite{attention:2017} presented the Transformer architecture and its principal component, 
the Attention Mechanism. The objective of this methodology is to permit the elements of the sequence to be accessed at any time, thereby reducing the loss of information.

\subsection{Self-attention mechanism}

Self-attention represents a key component of the model in which tokens' information interact with each other.
At each stage of the encoder, tokens exchange information with one another, gather contextual data, and refine their previous representations (Figure 2.3). 

For each input token trainable projections $W_Q$, $W_K$, $W_V$ obtained three representations interpreted by humans as a query ($q_i$), a key ($k_i$), and a value ($v_i$).
\begin{itemize}
    \item $q_i$ - query for other tokens about the information;
    \item $k_i$ - keys that will be used to extract information about the token;
    \item $v_i$ - values of information;
\end{itemize}

Next are the attention weights that will be applied to the values from which the important information will be extracted:
\begin{center}
    $Attn_Weights_{i} = softmax(\frac{q_i \cdot k_{1}^T}{C}, \frac{q_i \cdot k_{2}^T}{C}, \dots)$,
\end{center}

where $C$ is some normalization constant.

The Self-Attention result is a weighted sum of Values with coefficients in the form of attention weights. In vectorized form, you can write:

\begin{center}
    $Attention(q, k, v) = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V $
\end{center}
where $\sqrt[]{d}$ constant of the dimension of $K$ and $V$. 

The figure below shows the complete algorithm for the interaction of Q, K, V.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/query-key-value.png}
    \caption{Query, Key, Value in Self-Attention mechanism}
\end{figure}

In the decodert part the structure of the attention mechanism is different. 
They use Masked self-attention. In the form of attention described above, each token will "look" at the entire sequence, which is undesirable for the decoder. 

In the decoder, the generation of tokens occurs sequentially. If the attention mechanism has access to all tokens, not just those already used, this will result in information leakage in the decoder at the training stage and a reduction in the quality of the model.
To avoid this problem, Masked Self-Attention is used, which masks tokens that follow the current one and reduces their probability in the softmax function to 0.

\subsection{Multi-head attention}

To cover more information and dependencies, the authors propose the use of multiple levels of attention in parallel and the summarisation of the values obtained. 
This is referred to as Multi-head attention. The figure below illustrates what a multi-head approach looks like:

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{img/multi-head.png}
    \caption{Query, Key, Value in Self-Attention mechanism}
\end{figure}

\subsection{Feed Forward Network}

Transformer block also contains feed-forward network consisting of a linear layer and a activation function.
The FFN layer can be expressed as follows:
\begin{center}
    $FFN(x) = act(x \dot W_{1} + b_{1}) \dot W_{2} + b_{2}$, \\    
\end{center}
where $act$ is the activation function such as ReLU = $\max(0, x)$ or more contemporary GELU = $x\Phi(x)$.

In terms of LayerNorm, it is normalizes the vector representation of each sequence in the batch. 
The vector representation of each token is normalized in the Transformer. 
In addition, LayerNorm has trainable parameters, $scale$ and $bias$, which are used after normalization to scale the output of the layer.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/layer_norm.png}
    \caption{LayerNorm слой}
\end{figure}

\subsection{LayerNorm}

%do it

\subsection{Positional encodings and Rotary embedding}

A sequence of tokens is fed into the model, but the model itself does not understand in which order the tokens are located. 
Therefore, positional embeddings are added to the tokens, which are designed to show the model the input order of the elements.
Positional embeddings are given by formulas:
\begin{center}
    $PE_{position, 2i} = \sin(\frac{position}{10000^{2i/d_{model}}})$, \\
    $PE_{position, 2i+1} = \cos(\frac{position}{10000^{2i/d_{model}}})$, \\
\end{center}

Each measure of position encoding corresponds to a sin wave, and the wavelengths form a geometric progression from $2\pi$ to 10,000$\cdot 2\pi$.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{img/pos_encoding.png}
    \caption{Positional encoding}
\end{figure}

A newer version of positional encoding is rotary embeddings.
Rotary position embedding represents a distinct approach to incorporating relative position information into the attention matrix. 
This approach differs from other approaches in that it first multiplies queries and keys with a rotation matrix. This rotation matrix is a function of absolute position. 
Calculating the inner products of rotated queries $W_Q$ and keys $W_K$ results in an attention matrix that is a function of relative position information only.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{img/rotary_embs.png}
    \caption{Rotary embeddings}
\end{figure}

\section{BERT}
\subsection{Model overview}

The BERT is a language model based on the transformer architecture, that was first introduced in the article BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\cite{bert:2018}.  
From an architectural point of view, BERT consists of 12 encoder blocks, including a Self-Attention mechanism, Normalization and Feed-forward layers.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{img/bert.png}
    \caption{BERT architecture}
\end{figure}


\subsection{Training tasks}
BERT has two pretrain objective described in article: \\
The first objective of the training process is the masked language modelling (MLM). 
During the training phase of MLM, the following occurs:

\begin{itemize}
    \item A number of tokens are selected with a probability of 15\% for each token.
    \item The selected tokens are replaced by [MASK] with a probability of 80\%, a random token with a probability of 15\% and remain unchanged with a probability of 10\%.
    \item The model must predict the original token.
\end{itemize}

Secondly, Next Sentence Prediction (NSP) pretraining task is a binary classification task. 
In order to understand relationship between two sentences, BERT training process also uses next sentence prediction.
During training the model gets as input pairs of sentences and it learns to predict if the second sentence is the next sentence in the original text as well. 
The training dataset presented in the original paper states that when trained, 50\% of the examples contain related sentences extracted from the training texts, while the remaining 50\% contain a random pair of sentences.


\section{Large Language Model Meta AI}


\section{Mistral LLM}






The pipeline of dynamic thematic modelling of audio dialogues, which will be considered in this paper, is divided into several main stages. 
These include audio-to-text, dialogue segmentation, topic extraction, and topic evolution. 
The approaches and algorithms used in these stages will be described in detail from a theoretical point of view.

\section{Automatic Speech Recognition Stage}


\section{Dialogue Segmentation Stage}

\subsection{Utterance-Pair Coherence Scoring approach}
The dialogue segmentation stage represents the initial stage for textual type of data. 
The result is a set of utterance indices that indicate the beginning of a thematically related group. 

In accordance with the majority of previous studies, we have utilise the best open-source solution based on the presented in papers metrics. 
It is algorithm from Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring by Linzi Xing and Giuseppe Carenini (2020)
Let us now examine the algorithm in greater detail. 

In terms of our approach to topic segmentation, authors utilise Next Sentence Prediction (NSP) BERT as the encoder, due to the similar type of task. 
Input of both tasks an utterances, with the objective of predicting the appropriate next sentence, which should be topically related. 

In more detail, the positive instances $(s_{i}, s_{t+i})$ and negative instances $(s_{i}, s_{t-i})$ use in the model's fine-tuning stage  in the form described below: 
\begin{center}
    $[CLS] s_{i} [SEP] s_{t+/-i} [SEP]$,
\end{center}
where the symbols $[CLS]$ and $[SEP]$ represent special tokens in BERT. 

In order to indicate the thematic similarity of two statements in a dialogue, the $[CLS]$ token is employed, which performs a similar role in the usual BERT. 
The resulting value is subjected to a linear dictionary and a final similarity value is obtained, which is then utilised to calculate Coherence scores.

\subsection{Depth Scores}

With the help of a fine-tuned BERT (the training algorithm is described above) coherence scores $c_i$, where $i \in [0, n - 1]$ are calculate. According to texttiling algorithm description $c_i$ indicates the topical relatedness of the two adjacent utterances.
The model uses the obtained values as base values, but processes them for final predictions and gets Depth scores $ds_i$, where $i \in [0, n - 1]$  - measuring the sharpness of a valley.

The depth score is calculated as shows below: 
\begin{center}
    $D_{pi} = \frac{h_{l}(i) + h_{r}(i) - 2 \cdot c_{i}}{2}$,
\end{center}
where $h_{l}, h_{r}$ - the highest cs for $i$ pair in choosen range.

The greater the depth score between two statements, the less likely it is that they share a thematic relationship. 
To identify the pairs along which the boundaries of the thematic segments lie, the threshold value is calculated as follows: $threshold = \mu - \frac{\sigma}{2}$, where $\mu$ is the average value and $\sigma$ is the standard deviation. Pairs with a depth score above the $threshold$ value will be choosen as boundaries.

\subsection{Stage Evaluation}

After defining the basic segmentation algorithm, it is worth talking about evaluation metrics for this stage:
\begin{itemize}
    \item \textbf{F1}\\
    Given that this is a binary classification problem, it is tempting to use Precision and Recall, and their combination $F_{\beta}$ score as an evaluation metric. \\ 
     - Metric is defined as the boundaries ratio identified as true boundaries; \\
       $Precision = \frac{true \: positive}{true \: positive + false \: positive}$ \\
     - Recall is defined as the true boundaries ratio identified by the model. \\
       $Recall = \frac{true \: positive}{true \: positive + false \: negative}$ \\
    $F_{\beta}$ score is defined as the weighted harmonic mean of precision and recall: \\
    $F_{\beta} = (1 + \beta^{2}) \cdot \frac{precision \cdot recall}{(\beta^{2} \cdot precision) + recall}$
    \item \textbf{Pk score} is calculated using a sliding window-based method. The window size is typically set to half of the average true segment number. As the window is slide, the algorithm determines whether the two ends of the window are in the same or different segments in the ground truth segmentation. If there is a mismatch, the algorithm increases a counter. The final score is calculated by scaling the penalty between 0 and 1 and dividing the number of measurements.
    \item \textbf{WindowDiff} is also calculated by a sliding window. In this case, for each position of the window of size k, we simply compare the number of boundaries in the ground truth with the number of boundaries predicted by the Topic Segmentation model.\\
    $WD(pred, true) = \frac{1}{N - k}\sum_{i = 1}^{N - k}(\left\lvert b (true_{i}, true_{i + k}) - b(pred_{i}, pred_{i + k}) \right\rvert > 0)$, \\ 
    where $b(i, j)$ represents the number of boundaries between two positions $i$ and $j$ in the text, $N$ represents the number of sentences in the text.
\end{itemize} 

In practice, both $Pk$ and $WindowDiff$ scores are used to evaluate a model. 
A lower score means predictions are closer to the actual boundaries.

\section{Topic Extraction Stage}


\subsection{PromptTopic}


\subsection{LLM for topic extraction}

\subsection{Stage Evaluation}
As for the quality evaluation of the extracted topics, we will use two metrics: Topic coherence and Topic diversity
\begin{itemize}
    \item \textbf{Topic diversity} is a metric that assesses the ratio of unique keywords across all topic representations. This metric is used to measure the capabilities of a topic model covering a wide range of topics. The diversity score is a value between 0 and 1, with a score of 0 indicating repetitive topics and a score of 1 indicating diverse topics.
    \item \textbf{Topic coherence} is a measure of the degree of interconnection of words within a topic. In our experiments, we employed normalized point-to-point mutual information (PMI) as a metric for thematic consistency. A higher score for a given name indicates greater consistency, with an absolute correlation estimated at 1 point.
    \end{itemize}


\section{Topic Evolution over time}






\newpage
\chapter{Experiments}

The experiments will be divided into stages, with each stage accompanied by a description of the stage and the algorithms used. 
This description is included in the theoretical part of the work.

\subsection{Experiments settings}



\subsection{ASR experiments}



\subsection{Text clustering}

The input data at this stage are dialogues divided into separate utterances of the participants ${ut_1, ut_2, \dots, ut_n}$.



\subsection{Topic extraction}


\subsection{Topic Evolution}

\chapter{Limitations}

The LLM and other models require a significant amount of computing resources, including GPUs. Due to the limited availability of resources, the models were not larger than 8 billion parameters and did not exceed 16 GB during training and inference at all stages. 
Additionally, due to the lack of financial opportunities, open-source models were employed in the work. Should it be possible to reproduce all stages on more advanced models (such as GPT 4 or Lama 80b), the experimental results would be considerably more favourable.

\chapter{Conclusion}


\putbibliography

\end{document}